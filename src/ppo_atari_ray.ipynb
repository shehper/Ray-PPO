{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from stable_baselines3.common.atari_wrappers import (  # isort:skip\n",
    "    ClipRewardEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv,\n",
    ")\n",
    "\n",
    "# import ray\n",
    "# ray.init(log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n",
    "    #     help=\"the name of this experiment\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1,\n",
    "        help=\"seed of the experiment\")\n",
    "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, cuda will be enabled by default\")\n",
    "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
    "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n",
    "        help=\"the wandb's project name\")\n",
    "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
    "        help=\"the entity (team) of wandb's project\")\n",
    "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"whether to capture videos of the agent performances (check out `videos` folder)\")\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    parser.add_argument(\"--env-id\", type=str, default=\"BreakoutNoFrameskip-v4\",\n",
    "        help=\"the id of the environment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=10000000,\n",
    "        help=\"total timesteps of the experiments\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n",
    "        help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--num-envs\", type=int, default=8,\n",
    "        help=\"the number of parallel game environments\")\n",
    "    parser.add_argument(\"--num-steps\", type=int, default=128,\n",
    "        help=\"the number of steps to run in each environment per policy rollout\")\n",
    "    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggle learning rate annealing for policy and value networks\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "        help=\"the discount factor gamma\")\n",
    "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n",
    "        help=\"the lambda for the general advantage estimation\")\n",
    "    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n",
    "        help=\"the number of mini-batches\")\n",
    "    parser.add_argument(\"--update-epochs\", type=int, default=4,\n",
    "        help=\"the K epochs to update the policy\")\n",
    "    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles advantages normalization\")\n",
    "    parser.add_argument(\"--clip-coef\", type=float, default=0.1,\n",
    "        help=\"the surrogate clipping coefficient\")\n",
    "    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n",
    "    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n",
    "        help=\"coefficient of the entropy\")\n",
    "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
    "        help=\"coefficient of the value function\")\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
    "        help=\"the maximum norm for the gradient clipping\")\n",
    "    parser.add_argument(\"--target-kl\", type=float, default=None,\n",
    "        help=\"the target KL divergence threshold\")\n",
    "    args = parser.parse_args(\"\")\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    # fmt: on\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_init and Agent\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    #def __init__(self, obs_space_shape, action_space_n):\n",
    "    def __init__(self, action_space_n):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(512, action_space_n), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "        #return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        #logits = self.actor(x)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        #return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging data\n",
    "# @ray.remote\n",
    "class Logging_Data:\n",
    "    def __init__(self, run_name, args):\n",
    "        self.global_step = 0\n",
    "        if args.track:\n",
    "            import wandb\n",
    "            wandb.init(\n",
    "                project=args.wandb_project_name,\n",
    "                entity=args.wandb_entity,\n",
    "                sync_tensorboard=True,\n",
    "                config=vars(args),\n",
    "                name=run_name,\n",
    "                monitor_gym=True,\n",
    "                save_code=True,\n",
    "                mode=\"offline\"\n",
    "            )\n",
    "        self.writer = SummaryWriter(f\"atari-runs/{run_name}\")\n",
    "        self.writer.add_text(\n",
    "            \"hyperparameters\",\n",
    "            \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "        )\n",
    "\n",
    "    def increment_global_step(self):\n",
    "        self.global_step += 1\n",
    "\n",
    "    def log_data(self, data):\n",
    "        for key, value in data.items():\n",
    "            self.writer.add_scalar(key, value, self.global_step)\n",
    "\n",
    "    def get_global_step(self):\n",
    "        return self.global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollout\n",
    "# @ray.remote\n",
    "class Rollout:\n",
    "    def __init__(self, env_callable):\n",
    "        # random.seed(env_seed)\n",
    "        # np.random.seed(env_seed)\n",
    "        # torch.manual_seed(env_seed)\n",
    "        # torch.backends.cudnn.deterministic = args.torch_deterministic # not sure what this does\n",
    "        self.env = env_callable()\n",
    "        self.obs = torch.zeros((args.num_steps,) + self.env.observation_space.shape)\n",
    "        self.actions = torch.zeros((args.num_steps,) + self.env.action_space.shape)\n",
    "        self.logprobs = torch.zeros((args.num_steps,))\n",
    "        self.rewards = torch.zeros((args.num_steps,))\n",
    "        self.dones = torch.zeros((args.num_steps,))\n",
    "        self.values = torch.zeros((args.num_steps,))\n",
    "        self.advantages = torch.zeros((args.num_steps,))\n",
    "        self.returns = torch.zeros((args.num_steps,))\n",
    "        \n",
    "        self.next_obs = torch.Tensor(self.env.reset()).to(device)\n",
    "        self.next_done = torch.zeros(1).to(device)\n",
    "\n",
    "        # self.episode_return = 0\n",
    "        # self.episode_length = 0\n",
    "\n",
    "    def get_env_spaces_data(self):\n",
    "        return self.env.observation_space.shape, self.env.action_space.n\n",
    "        \n",
    "    def rollout(self, agent, logging_data):\n",
    "        for step in range(args.num_steps):\n",
    "            #ray.get(logging_data.increment_global_step.remote())\n",
    "            logging_data.increment_global_step()\n",
    "            self.obs[step] = self.next_obs\n",
    "            self.dones[step] = self.next_done\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(torch.unsqueeze(self.next_obs,dim=0))\n",
    "                self.values[step] = value.flatten() # num_envs\n",
    "            self.actions[step] = action\n",
    "            self.logprobs[step] = logprob\n",
    "\n",
    "            self.next_obs, reward, done, info = self.env.step(torch.squeeze(action).cpu().numpy())\n",
    "            self.rewards[step] = torch.tensor(reward).to(device).view(-1) # different\n",
    "            self.next_obs = torch.Tensor(self.next_obs).to(device)\n",
    "            self.next_done = torch.Tensor([done]).to(device) # different\n",
    "            \n",
    "            # self.episode_return += reward\n",
    "            # self.episode_length += 1\n",
    "\n",
    "            ## EpisodicLifeEnv and MaxandSkipEnv change the done condition. \n",
    "            # So we cannot just use 'if done' to record statistics. \n",
    "            # Instead, we filter for 'done's that were originally settled at\n",
    "            # the level of original Atari env in the code line gym.make(env_id).\n",
    "            # This is preserved by looking for 'episode' key in info.\n",
    "\n",
    "            # for item in info:\n",
    "            #     if \"episode\" in item.keys():\n",
    "            #         print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "\n",
    "            if 'episode' in info.keys():\n",
    "                global_step = logging_data.get_global_step()\n",
    "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                print(info)\n",
    "                self.env.reset()\n",
    "\n",
    "            # if done:\n",
    "            #     print(info)\n",
    "                # # global_step = ray.get(logging_data.get_global_step.remote())\n",
    "                # global_step = logging_data.get_global_step()\n",
    "                # print(f\"global_step={global_step}, episodic_return={self.episode_return}\")\n",
    "                # # ray.get(logging_data.log_data.remote(\n",
    "                # #     {\"charts/episodic_return\": self.episode_return,\n",
    "                # #      \"charts/episodic_length\": self.episode_length}\n",
    "                # # ))\n",
    "                # logging_data.log_data(\n",
    "                #     {\"charts/episodic_return\": self.episode_return,\n",
    "                #      \"charts/episodic_length\": self.episode_length}\n",
    "                # )\n",
    "                # self.episode_length = self.episode_return = 0\n",
    "                # self.env.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #self.next_obs = torch.unsqueeze(self.next_obs, dim=0)\n",
    "            next_value = agent.get_value(torch.unsqueeze(self.next_obs, dim=0)).reshape(1, -1)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - self.next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - self.dones[t + 1]\n",
    "                    nextvalues = self.values[t + 1]\n",
    "                delta = self.rewards[t] + args.gamma * nextvalues * nextnonterminal - self.values[t]\n",
    "                self.advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            self.returns = self.advantages + self.values \n",
    "\n",
    "        rollout_data = {'obs': self.obs,\n",
    "             'actions': self.actions, \n",
    "             'logprobs': self.logprobs,\n",
    "             'values': self.values,\n",
    "             'returns': self.returns,\n",
    "             'advantages': self.advantages}\n",
    "\n",
    "        return rollout_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(agent, optimizer, rollout_data, args):\n",
    "\n",
    "    b_inds = np.arange(args.batch_size) # indices of batch_size\n",
    "    b_obs = torch.cat([result['obs'] for result in rollout_data], axis=0)\n",
    "    b_logprobs = torch.cat([result['logprobs'] for result in rollout_data], axis=0)\n",
    "    b_actions = torch.cat([result['actions'] for result in rollout_data], axis=0)\n",
    "    b_advantages = torch.cat([result['advantages'] for result in rollout_data], axis=0)\n",
    "    b_returns = torch.cat([result['returns'] for result in rollout_data], axis=0)\n",
    "    b_values = torch.cat([result['values'] for result in rollout_data], axis=0)\n",
    "\n",
    "    clipfracs = []\n",
    "\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "\n",
    "        for start in range(0, args.batch_size, args.minibatch_size): \n",
    "            end = start + args.minibatch_size \n",
    "            mb_inds = b_inds[start:end] # indices of minibatch\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds]) #.long() converts dtype to int64\n",
    "            logratio = newlogprob - b_logprobs[mb_inds] \n",
    "            ratio = logratio.exp() # pi(a|s) / pi_old(a|s); is a tensor of 1s for epoch=0.\n",
    "\n",
    "            with torch.no_grad():\n",
    "                approx_kl = ((ratio - 1) - logratio).mean() # mean of (pi(a|s) / pi_old(a|s) - 1 - log(pi(a|s) / pi_old(a|s)))\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]  \n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv: \n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1) # value computed by NN with updated parameters\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm) # clip gradients before updating them.\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None:\n",
    "            if approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy() \n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    output = {\"pg_loss\": pg_loss.item(), \"v_loss\": v_loss.item(), \"entropy_loss\": entropy_loss.item(), \n",
    "              \"approx_kl\": approx_kl, \"explained_var\": explained_var, \"clipfrac\": np.mean(clipfracs)}\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=762, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 507, 'frame_number': 507, 'episode': {'r': 0.0, 'l': 508, 't': 3.856217}}\n",
      "global_step=1024, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 540, 'frame_number': 540, 'episode': {'r': 0.0, 'l': 541, 't': 4.685488}}\n",
      "update:  1  SPS:  87\n",
      "global_step=1026, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 540, 'frame_number': 540, 'episode': {'r': 0.0, 'l': 541, 't': 12.968323}}\n",
      "global_step=1199, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 731, 'frame_number': 731, 'episode': {'r': 1.0, 'l': 732, 't': 13.575513}}\n",
      "global_step=1479, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 823, 'frame_number': 823, 'episode': {'r': 2.0, 'l': 824, 't': 14.493676}}\n",
      "update:  2  SPS:  87\n",
      "global_step=2133, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 866, 'frame_number': 1406, 'episode': {'r': 2.0, 'l': 867, 't': 24.985654}}\n",
      "global_step=2297, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 822, 'frame_number': 1553, 'episode': {'r': 2.0, 'l': 823, 't': 25.569998}}\n",
      "global_step=2406, episodic_return=5.0\n",
      "{'lives': 0, 'episode_frame_number': 1464, 'frame_number': 1464, 'episode': {'r': 5.0, 'l': 1465, 't': 25.907657}}\n",
      "global_step=2722, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 694, 'frame_number': 1201, 'episode': {'r': 1.0, 'l': 695, 't': 26.829001}}\n",
      "global_step=2939, episodic_return=6.0\n",
      "{'lives': 0, 'episode_frame_number': 1552, 'frame_number': 1552, 'episode': {'r': 6.0, 'l': 1553, 't': 27.617422}}\n",
      "global_step=2990, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 712, 'frame_number': 1252, 'episode': {'r': 1.0, 'l': 713, 't': 27.694047}}\n",
      "update:  3  SPS:  87\n",
      "global_step=3478, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 862, 'frame_number': 1685, 'episode': {'r': 2.0, 'l': 863, 't': 37.697357}}\n",
      "global_step=3688, episodic_return=14.0\n",
      "{'lives': 0, 'episode_frame_number': 1977, 'frame_number': 1977, 'episode': {'r': 14.0, 'l': 1978, 't': 38.460901}}\n",
      "global_step=3806, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 773, 'frame_number': 1974, 'episode': {'r': 2.0, 'l': 774, 't': 38.835749}}\n",
      "global_step=4088, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 820, 'frame_number': 2072, 'episode': {'r': 2.0, 'l': 821, 't': 39.738244}}\n",
      "update:  4  SPS:  87\n",
      "global_step=4103, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 741, 'frame_number': 2147, 'episode': {'r': 1.0, 'l': 742, 't': 48.179372}}\n",
      "global_step=4272, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 741, 'frame_number': 2294, 'episode': {'r': 1.0, 'l': 742, 't': 48.846859}}\n",
      "global_step=4427, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 924, 'frame_number': 2388, 'episode': {'r': 3.0, 'l': 925, 't': 49.436552}}\n",
      "global_step=4539, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 693, 'frame_number': 2378, 'episode': {'r': 1.0, 'l': 694, 't': 49.807304}}\n",
      "global_step=4728, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 606, 'frame_number': 2583, 'episode': {'r': 0.0, 'l': 607, 't': 50.513781}}\n",
      "global_step=4848, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 600, 'frame_number': 2574, 'episode': {'r': 0.0, 'l': 601, 't': 50.913181}}\n",
      "global_step=4887, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 648, 'frame_number': 2200, 'episode': {'r': 1.0, 'l': 649, 't': 50.95691}}\n",
      "update:  5  SPS:  86\n",
      "global_step=5131, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 562, 'frame_number': 2709, 'episode': {'r': 0.0, 'l': 563, 't': 60.179165}}\n",
      "global_step=5368, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 816, 'frame_number': 3110, 'episode': {'r': 2.0, 'l': 817, 't': 61.026737}}\n",
      "global_step=5502, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 727, 'frame_number': 3115, 'episode': {'r': 1.0, 'l': 728, 't': 61.448285}}\n",
      "global_step=5567, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 549, 'frame_number': 2927, 'episode': {'r': 0.0, 'l': 550, 't': 61.58439}}\n",
      "global_step=5879, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 574, 'frame_number': 3148, 'episode': {'r': 0.0, 'l': 575, 't': 62.598573}}\n",
      "global_step=6040, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 675, 'frame_number': 2747, 'episode': {'r': 1.0, 'l': 676, 't': 62.971927}}\n",
      "update:  6  SPS:  87\n",
      "global_step=6187, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 647, 'frame_number': 3356, 'episode': {'r': 1.0, 'l': 648, 't': 71.796568}}\n",
      "global_step=6653, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 796, 'frame_number': 3723, 'episode': {'r': 1.0, 'l': 797, 't': 73.313189}}\n",
      "global_step=6950, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1112, 'frame_number': 3312, 'episode': {'r': 3.0, 'l': 1113, 't': 74.120988}}\n",
      "global_step=7081, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 591, 'frame_number': 3338, 'episode': {'r': 0.0, 'l': 592, 't': 74.532696}}\n",
      "update:  7  SPS:  87\n",
      "global_step=7208, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 511, 'frame_number': 3867, 'episode': {'r': 0.0, 'l': 512, 't': 83.336721}}\n",
      "global_step=7427, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 545, 'frame_number': 3660, 'episode': {'r': 0.0, 'l': 546, 't': 84.043121}}\n",
      "global_step=7677, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 548, 'frame_number': 4271, 'episode': {'r': 0.0, 'l': 549, 't': 85.059417}}\n",
      "global_step=7717, episodic_return=4.0\n",
      "{'lives': 0, 'episode_frame_number': 1210, 'frame_number': 3793, 'episode': {'r': 4.0, 'l': 1211, 't': 85.105536}}\n",
      "global_step=7920, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1013, 'frame_number': 4161, 'episode': {'r': 3.0, 'l': 1014, 't': 85.892606}}\n",
      "global_step=8149, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 705, 'frame_number': 4043, 'episode': {'r': 1.0, 'l': 706, 't': 86.588541}}\n",
      "update:  8  SPS:  87\n",
      "global_step=8358, episodic_return=4.0\n",
      "{'lives': 0, 'episode_frame_number': 1226, 'frame_number': 4336, 'episode': {'r': 4.0, 'l': 1227, 't': 95.629887}}\n",
      "global_step=8462, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 567, 'frame_number': 4227, 'episode': {'r': 0.0, 'l': 568, 't': 95.933057}}\n",
      "global_step=8698, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 535, 'frame_number': 4806, 'episode': {'r': 0.0, 'l': 536, 't': 96.778549}}\n",
      "global_step=8991, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1028, 'frame_number': 4340, 'episode': {'r': 3.0, 'l': 1029, 't': 97.572196}}\n",
      "update:  9  SPS:  87\n",
      "global_step=9337, episodic_return=5.0\n",
      "{'lives': 0, 'episode_frame_number': 1378, 'frame_number': 5245, 'episode': {'r': 5.0, 'l': 1379, 't': 107.110365}}\n",
      "global_step=9463, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 866, 'frame_number': 5202, 'episode': {'r': 2.0, 'l': 867, 't': 107.504669}}\n",
      "global_step=9759, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1032, 'frame_number': 4825, 'episode': {'r': 3.0, 'l': 1033, 't': 108.324701}}\n",
      "global_step=9919, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 853, 'frame_number': 5014, 'episode': {'r': 2.0, 'l': 854, 't': 108.856342}}\n",
      "global_step=10141, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 825, 'frame_number': 4868, 'episode': {'r': 2.0, 'l': 826, 't': 109.490463}}\n",
      "update:  10  SPS:  87\n",
      "global_step=10528, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1128, 'frame_number': 5355, 'episode': {'r': 3.0, 'l': 1129, 't': 119.048786}}\n",
      "global_step=10713, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 902, 'frame_number': 5708, 'episode': {'r': 2.0, 'l': 903, 't': 119.683255}}\n",
      "global_step=10832, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 728, 'frame_number': 5553, 'episode': {'r': 1.0, 'l': 729, 't': 120.042159}}\n",
      "global_step=11037, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1041, 'frame_number': 5381, 'episode': {'r': 3.0, 'l': 1042, 't': 120.616368}}\n",
      "global_step=11213, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 733, 'frame_number': 5601, 'episode': {'r': 1.0, 'l': 734, 't': 121.201153}}\n",
      "update:  11  SPS:  87\n",
      "global_step=11433, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 731, 'frame_number': 5933, 'episode': {'r': 1.0, 'l': 732, 't': 130.214936}}\n",
      "global_step=11549, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 532, 'frame_number': 5887, 'episode': {'r': 0.0, 'l': 533, 't': 130.560403}}\n",
      "global_step=11736, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 527, 'frame_number': 6235, 'episode': {'r': 0.0, 'l': 528, 't': 131.201038}}\n",
      "global_step=11960, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1013, 'frame_number': 6027, 'episode': {'r': 3.0, 'l': 1014, 't': 131.848396}}\n",
      "global_step=12143, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 870, 'frame_number': 6251, 'episode': {'r': 2.0, 'l': 871, 't': 132.463199}}\n",
      "global_step=12260, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 629, 'frame_number': 6230, 'episode': {'r': 0.0, 'l': 630, 't': 132.805809}}\n",
      "update:  12  SPS:  87\n",
      "global_step=12538, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 844, 'frame_number': 6777, 'episode': {'r': 2.0, 'l': 845, 't': 142.369045}}\n",
      "global_step=12573, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 535, 'frame_number': 6422, 'episode': {'r': 0.0, 'l': 536, 't': 142.393045}}\n",
      "global_step=12764, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 552, 'frame_number': 6787, 'episode': {'r': 0.0, 'l': 553, 't': 143.083359}}\n",
      "global_step=12826, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 830, 'frame_number': 6383, 'episode': {'r': 2.0, 'l': 831, 't': 143.212797}}\n",
      "update:  13  SPS:  87\n",
      "global_step=13313, episodic_return=6.0\n",
      "{'lives': 0, 'episode_frame_number': 1601, 'frame_number': 6846, 'episode': {'r': 6.0, 'l': 1602, 't': 153.216601}}\n",
      "global_step=13563, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 532, 'frame_number': 7309, 'episode': {'r': 0.0, 'l': 533, 't': 154.148081}}\n",
      "global_step=13595, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 511, 'frame_number': 6933, 'episode': {'r': 0.0, 'l': 512, 't': 154.270895}}\n",
      "global_step=14008, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 1043, 'frame_number': 7070, 'episode': {'r': 3.0, 'l': 1044, 't': 155.722541}}\n",
      "global_step=14098, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 683, 'frame_number': 6934, 'episode': {'r': 1.0, 'l': 684, 't': 155.995908}}\n",
      "global_step=14263, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 876, 'frame_number': 7106, 'episode': {'r': 2.0, 'l': 877, 't': 156.627144}}\n",
      "update:  14  SPS:  87\n",
      "global_step=14430, episodic_return=3.0\n",
      "{'lives': 0, 'episode_frame_number': 897, 'frame_number': 7743, 'episode': {'r': 3.0, 'l': 898, 't': 165.845331}}\n",
      "global_step=14619, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 521, 'frame_number': 7454, 'episode': {'r': 0.0, 'l': 522, 't': 166.43653}}\n",
      "global_step=14755, episodic_return=2.0\n",
      "{'lives': 0, 'episode_frame_number': 810, 'frame_number': 7597, 'episode': {'r': 2.0, 'l': 811, 't': 166.914986}}\n",
      "global_step=14863, episodic_return=4.0\n",
      "{'lives': 0, 'episode_frame_number': 1006, 'frame_number': 7389, 'episode': {'r': 4.0, 'l': 1007, 't': 167.270636}}\n",
      "global_step=15036, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 563, 'frame_number': 7633, 'episode': {'r': 0.0, 'l': 564, 't': 167.896217}}\n",
      "global_step=15124, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 533, 'frame_number': 7467, 'episode': {'r': 0.0, 'l': 534, 't': 168.140976}}\n",
      "global_step=15284, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 507, 'frame_number': 7613, 'episode': {'r': 0.0, 'l': 508, 't': 168.701979}}\n",
      "update:  15  SPS:  87\n",
      "global_step=15537, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 752, 'frame_number': 8061, 'episode': {'r': 1.0, 'l': 753, 't': 178.005484}}\n",
      "global_step=15704, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 771, 'frame_number': 8225, 'episode': {'r': 1.0, 'l': 772, 't': 178.621841}}\n",
      "global_step=15826, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 717, 'frame_number': 8314, 'episode': {'r': 1.0, 'l': 718, 't': 179.038786}}\n",
      "global_step=16074, episodic_return=0.0\n",
      "{'lives': 0, 'episode_frame_number': 579, 'frame_number': 8212, 'episode': {'r': 0.0, 'l': 580, 't': 179.923528}}\n",
      "global_step=16191, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 692, 'frame_number': 8159, 'episode': {'r': 1.0, 'l': 693, 't': 180.420108}}\n",
      "update:  16  SPS:  87\n",
      "global_step=16404, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 743, 'frame_number': 8486, 'episode': {'r': 1.0, 'l': 744, 't': 189.592657}}\n",
      "global_step=16895, episodic_return=1.0\n",
      "{'lives': 0, 'episode_frame_number': 716, 'frame_number': 9030, 'episode': {'r': 1.0, 'l': 717, 't': 191.718639}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lrnow\n\u001b[1;32m     34\u001b[0m \u001b[39m# Collecting data through N parallel actors\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# rollout_data = ray.get([env.rollout.remote(agent, logging_data) for env in envs])\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m rollout_data \u001b[39m=\u001b[39m [env\u001b[39m.\u001b[39mrollout(agent, logging_data) \u001b[39mfor\u001b[39;00m env \u001b[39min\u001b[39;00m envs]\n\u001b[1;32m     38\u001b[0m \u001b[39m# Updating parameters of neural networks\u001b[39;00m\n\u001b[1;32m     39\u001b[0m output \u001b[39m=\u001b[39m update_parameters(agent, optimizer, rollout_data, args)\n",
      "Cell \u001b[0;32mIn[40], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lrnow\n\u001b[1;32m     34\u001b[0m \u001b[39m# Collecting data through N parallel actors\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# rollout_data = ray.get([env.rollout.remote(agent, logging_data) for env in envs])\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m rollout_data \u001b[39m=\u001b[39m [env\u001b[39m.\u001b[39;49mrollout(agent, logging_data) \u001b[39mfor\u001b[39;00m env \u001b[39min\u001b[39;00m envs]\n\u001b[1;32m     38\u001b[0m \u001b[39m# Updating parameters of neural networks\u001b[39;00m\n\u001b[1;32m     39\u001b[0m output \u001b[39m=\u001b[39m update_parameters(agent, optimizer, rollout_data, args)\n",
      "Cell \u001b[0;32mIn[38], line 41\u001b[0m, in \u001b[0;36mRollout.rollout\u001b[0;34m(self, agent, logging_data)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[step] \u001b[39m=\u001b[39m action\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogprobs[step] \u001b[39m=\u001b[39m logprob\n\u001b[0;32m---> 41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(torch\u001b[39m.\u001b[39;49msqueeze(action)\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards[step] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(reward)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# different\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_obs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_obs)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/work/RL-algorithms/env/lib/python3.9/site-packages/gym/wrappers/frame_stack.py:117\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 117\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes\u001b[39m.\u001b[39mappend(observation)\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(), reward, done, info\n",
      "File \u001b[0;32m~/work/RL-algorithms/env/lib/python3.9/site-packages/gym/core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 314\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/work/RL-algorithms/env/lib/python3.9/site-packages/gym/core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 314\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/work/RL-algorithms/env/lib/python3.9/site-packages/gym/core.py:327\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 327\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward(reward), done, info\n",
      "File \u001b[0;32m~/work/RL-algorithms/env/lib/python3.9/site-packages/gym/core.py:280\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[ObsType, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m--> 280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/work/RL-algorithms/env/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py:83\u001b[0m, in \u001b[0;36mEpisodicLifeEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GymStepReturn:\n\u001b[0;32m---> 83\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwas_real_done \u001b[39m=\u001b[39m done\n\u001b[1;32m     85\u001b[0m     \u001b[39m# check current lives, make loss of life terminal,\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[39m# then update lives to handle bonus lives\u001b[39;00m\n",
      "File \u001b[0;32m~/work/RL-algorithms/env/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py:138\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    136\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    137\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_skip):\n\u001b[1;32m    139\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "# run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "run_name = f\"{args.env_id}__{args.seed}__{int(time.time())}\"\n",
    "\n",
    "# logging_data = Logging_Data.remote(run_name, args)\n",
    "logging_data = Logging_Data(run_name, args)\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# envs = [Rollout.remote(make_env(args.env_id, args.seed + i, i, args.capture_video, run_name)) for i in range(args.num_envs)]\n",
    "# _, action_space_n = ray.get(envs[0].get_env_spaces_data.remote())\n",
    "envs = [Rollout(make_env(args.env_id, args.seed + i, i, args.capture_video, run_name)) for i in range(args.num_envs)]\n",
    "_, action_space_n = envs[0].get_env_spaces_data()\n",
    "agent = Agent(action_space_n).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "num_updates = args.total_timesteps // args.batch_size\n",
    "\n",
    "for update in range(1, num_updates+1):\n",
    "    # Annealing the learning rate if required\n",
    "    if args.anneal_lr:\n",
    "        frac = 1.0 - (update - 1.0) / num_updates\n",
    "        lrnow = frac * args.learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    # Collecting data through N parallel actors\n",
    "    # rollout_data = ray.get([env.rollout.remote(agent, logging_data) for env in envs])\n",
    "    rollout_data = [env.rollout(agent, logging_data) for env in envs]\n",
    "    \n",
    "    # Updating parameters of neural networks\n",
    "    output = update_parameters(agent, optimizer, rollout_data, args)\n",
    "    global_step = logging_data.get_global_step()\n",
    "    SPS = int(global_step / (time.time() - start_time))\n",
    "\n",
    "    # ray.get(logging_data.log_data.remote(\n",
    "    #             {\"charts/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "    #                 \"losses/value_loss\": output['v_loss'],\n",
    "    #                 \"losses/policy_loss\": output['pg_loss'],\n",
    "    #                 \"losses/entropy\": output['entropy_loss'],\n",
    "    #                 \"losses/approx_kl\": output['approx_kl'],\n",
    "    #                 \"losses/clipfrac\": output['clipfrac'],\n",
    "    #                 \"losses/explained_variance\": output['clipfrac'],\n",
    "    #                 \"charts/SPS\": SPS\n",
    "    #                 }\n",
    "    #         ))\n",
    "    print(\"update: \", update, \" SPS: \", SPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,4,84,84)\n",
    "_, _, _, action = agent.get_action_and_value(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 84, 84])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([3]),\n",
       " tensor([-1.3863], grad_fn=<SqueezeBackward1>),\n",
       " tensor([1.3863], grad_fn=<NegBackward0>),\n",
       " tensor([[0.0047]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_action_and_value(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
