{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {'a':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict.get('b') or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(np.array([[1,2], [3, 4]]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    PPOConfig()\n",
    "    .environment(\"BreakoutNoFrameskip-v4\")\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,\n",
    "\n",
    "algo.evaluate()  # 4. and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new modules installed to make rllib work: dm-tree, scipy, typer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rllib train file https://github.com/ray-project/rl-experiments/blob/master/atari-ppo/2018-09/atari-ppo.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 09:36:59,013\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "2023-08-25 09:36:59,052\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 09:37:02,817\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/tune/experiment/experiment.py:167: UserWarning: The `local_dir` argument of `Experiment is deprecated. Use `storage_path` or set the `TUNE_RESULT_DIR` environment variable instead.\n",
      "  warnings.warn(\n",
      "2023-08-25 09:37:04,291\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 09:37:04,296\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 09:37:04,347\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-08-25 09:37:04,876\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-25 09:37:04,901\tINFO tune.py:666 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2023-08-25 09:37:04,901\tWARNING syncer.py:260 -- You are using remote storage, but you don't have `fsspec` installed. This can lead to inefficient syncing behavior. To avoid this, install fsspec with `pip install fsspec`. Depending on your remote storage provider, consider installing the respective fsspec-package (see https://github.com/fsspec).\n",
      "2023-08-25 09:37:04,902\tINFO tensorboardx.py:178 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-08-25 09:37:04,902\tWARNING callback.py:144 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
      "╭────────────────────────────────────────────────────────╮\n",
      "│ Configuration for experiment     default               │\n",
      "├────────────────────────────────────────────────────────┤\n",
      "│ Search algorithm                 BasicVariantGenerator │\n",
      "│ Scheduler                        FIFOScheduler         │\n",
      "│ Number of trials                 1                     │\n",
      "╰────────────────────────────────────────────────────────╯\n",
      "\n",
      "View detailed results here: /Users/alishehper/ray_results/default\n",
      "\n",
      "Trial status: 1 PENDING\n",
      "Current time: 2023-08-25 09:37:05. Total running time: 0s\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "╭────────────────────────────────────────╮\n",
      "│ Trial name                    status   │\n",
      "├────────────────────────────────────────┤\n",
      "│ A2C_CartPole-v0_7aa12_00000   PENDING  │\n",
      "╰────────────────────────────────────────╯\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=46147)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=46147)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=46147)\u001b[0m 2023-08-25 09:37:08,462\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=46147)\u001b[0m 2023-08-25 09:37:08,462\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=46147)\u001b[0m 2023-08-25 09:37:08,462\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m /Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/envs/registration.py:523: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m   logger.deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,309\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,309\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,338\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,338\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,338\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,338\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,338\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,338\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46153)\u001b[0m 2023-08-25 09:37:11,350\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!\n",
      "Training started with configuration:\n",
      "╭─────────────────────────────────╮\n",
      "│ Training config                 │\n",
      "├─────────────────────────────────┤\n",
      "│ env                 CartPole-v0 │\n",
      "╰─────────────────────────────────╯\n",
      "\n",
      "\u001b[2m\u001b[36m(A2C pid=46147)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(A2C pid=46147)\u001b[0m 2023-08-25 09:37:11,568\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-25 09:37:11,836 E 46119 13267253] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-08-25_09-36-59_703066_46101 is over 95% full, available space: 11536392192; capacity: 245107195904. Object creation will fail if spilling is required.\n",
      "Training finished iteration 1 at 2023-08-25 09:37:21. Total running time: 16s\n",
      "╭─────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training result                                                                             │\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ episodes_total                                                                          324 │\n",
      "│ time_this_iter_s                                                                   10.00994 │\n",
      "│ time_total_s                                                                       10.00994 │\n",
      "│ timesteps_total                                                                       15200 │\n",
      "│ training_iteration                                                                        1 │\n",
      "│ agent_timesteps_total                                                                 15200 │\n",
      "│ connector_metrics/ObsPreprocessorConnector_ms                         0.0031948825459421418 │\n",
      "│ connector_metrics/StateBufferConnector_ms                             0.0027180453877390168 │\n",
      "│ connector_metrics/ViewRequirementAgentConnector_ms                      0.06618256922121402 │\n",
      "│ counters/num_agent_steps_sampled                                                      15200 │\n",
      "│ counters/num_agent_steps_trained                                                      15200 │\n",
      "│ counters/num_env_steps_sampled                                                        15200 │\n",
      "│ counters/num_env_steps_trained                                                        15200 │\n",
      "│ episode_len_mean                                                                   46.78704 │\n",
      "│ episode_reward_max                                                                     200. │\n",
      "│ episode_reward_mean                                                                46.78704 │\n",
      "│ episode_reward_min                                                                       8. │\n",
      "│ episodes_this_iter                                                                      324 │\n",
      "│ hist_stats/episode_lengths                                             ... 62, 77, 72, 101] │\n",
      "│ hist_stats/episode_reward                                              ...7.0, 72.0, 101.0] │\n",
      "│ info/learner/default_policy/diff_num_grad_updates_vs_sampler_policy                   474.0 │\n",
      "│ info/learner/default_policy/learner_stats/allreduce_latency                             0.0 │\n",
      "│ info/learner/default_policy/learner_stats/cur_lr                                     0.0001 │\n",
      "│ info/learner/default_policy/learner_stats/entropy_coeff                                0.01 │\n",
      "│ info/learner/default_policy/learner_stats/grad_gnorm                                   40.0 │\n",
      "│ info/learner/default_policy/learner_stats/policy_entropy                 19.670656204223633 │\n",
      "│ info/learner/default_policy/learner_stats/policy_loss                      95.8214111328125 │\n",
      "│ info/learner/default_policy/learner_stats/vf_loss                         881.1241455078125 │\n",
      "│ info/learner/default_policy/num_agent_steps_trained                                    32.0 │\n",
      "│ info/learner/default_policy/num_grad_updates_lifetime                                 475.0 │\n",
      "│ info/num_agent_steps_sampled                                                          15200 │\n",
      "│ info/num_agent_steps_trained                                                          15200 │\n",
      "│ info/num_env_steps_sampled                                                            15200 │\n",
      "│ info/num_env_steps_trained                                                            15200 │\n",
      "│ num_agent_steps_sampled                                                               15200 │\n",
      "│ num_agent_steps_trained                                                               15200 │\n",
      "│ num_env_steps_sampled                                                                 15200 │\n",
      "│ num_env_steps_sampled_this_iter                                                       15200 │\n",
      "│ num_env_steps_sampled_throughput_per_sec                                         1519.44498 │\n",
      "│ num_env_steps_trained                                                                 15200 │\n",
      "│ num_env_steps_trained_this_iter                                                       15200 │\n",
      "│ num_env_steps_trained_throughput_per_sec                                         1519.44498 │\n",
      "│ num_faulty_episodes                                                                       0 │\n",
      "│ num_healthy_workers                                                                       2 │\n",
      "│ num_in_flight_async_reqs                                                                  0 │\n",
      "│ num_remote_worker_restarts                                                                0 │\n",
      "│ num_steps_trained_this_iter                                                           15200 │\n",
      "│ perf/cpu_util_percent                                                     38.93999999999999 │\n",
      "│ perf/ram_util_percent                                                     96.35999999999999 │\n",
      "│ sampler_perf/mean_action_processing_ms                                  0.06832274123625622 │\n",
      "│ sampler_perf/mean_env_render_ms                                                         0.0 │\n",
      "│ sampler_perf/mean_env_wait_ms                                           0.03264411723579991 │\n",
      "│ sampler_perf/mean_inference_ms                                           0.4402931694532283 │\n",
      "│ sampler_perf/mean_raw_obs_processing_ms                                 0.23572374787876382 │\n",
      "│ sampler_results/connector_metrics/ObsPreprocessorConnector_ms         0.0031948825459421418 │\n",
      "│ sampler_results/connector_metrics/StateBufferConnector_ms             0.0027180453877390168 │\n",
      "│ sampler_results/connector_metrics/ViewRequirementAgentConnector_ms      0.06618256922121402 │\n",
      "│ sampler_results/episode_len_mean                                          46.78703703703704 │\n",
      "│ sampler_results/episode_reward_max                                                    200.0 │\n",
      "│ sampler_results/episode_reward_mean                                       46.78703703703704 │\n",
      "│ sampler_results/episode_reward_min                                                      8.0 │\n",
      "│ sampler_results/episodes_this_iter                                                      324 │\n",
      "│ sampler_results/hist_stats/episode_lengths                             ... 62, 77, 72, 101] │\n",
      "│ sampler_results/hist_stats/episode_reward                              ...7.0, 72.0, 101.0] │\n",
      "│ sampler_results/num_faulty_episodes                                                       0 │\n",
      "│ sampler_results/sampler_perf/mean_action_processing_ms                  0.06832274123625622 │\n",
      "│ sampler_results/sampler_perf/mean_env_render_ms                                         0.0 │\n",
      "│ sampler_results/sampler_perf/mean_env_wait_ms                           0.03264411723579991 │\n",
      "│ sampler_results/sampler_perf/mean_inference_ms                           0.4402931694532283 │\n",
      "│ sampler_results/sampler_perf/mean_raw_obs_processing_ms                 0.23572374787876382 │\n",
      "│ timers/learn_throughput                                                            9536.166 │\n",
      "│ timers/learn_time_ms                                                                  3.356 │\n",
      "│ timers/load_throughput                                                           207863.912 │\n",
      "│ timers/load_time_ms                                                                   0.154 │\n",
      "│ timers/sample_time_ms                                                                17.117 │\n",
      "│ timers/synch_weights_time_ms                                                          1.334 │\n",
      "│ timers/training_iteration_time_ms                                                    22.008 │\n",
      "╰─────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-25 09:37:21,927 E 46119 13267253] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-08-25_09-36-59_703066_46101 is over 95% full, available space: 11536207872; capacity: 245107195904. Object creation will fail if spilling is required.\n",
      "Training finished iteration 2 at 2023-08-25 09:37:31. Total running time: 26s\n",
      "╭─────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training result                                                                             │\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ episodes_total                                                                          515 │\n",
      "│ time_this_iter_s                                                                   10.02368 │\n",
      "│ time_total_s                                                                       20.03362 │\n",
      "│ timesteps_total                                                                       30976 │\n",
      "│ training_iteration                                                                        2 │\n",
      "│ agent_timesteps_total                                                                 30976 │\n",
      "│ connector_metrics/ObsPreprocessorConnector_ms                         0.0030443930501089047 │\n",
      "│ connector_metrics/StateBufferConnector_ms                              0.002707611203817797 │\n",
      "│ connector_metrics/ViewRequirementAgentConnector_ms                      0.06659342980509653 │\n",
      "│ counters/num_agent_steps_sampled                                                      30976 │\n",
      "│ counters/num_agent_steps_trained                                                      30976 │\n",
      "│ counters/num_env_steps_sampled                                                        30976 │\n",
      "│ counters/num_env_steps_trained                                                        30976 │\n",
      "│ episode_len_mean                                                                   82.32984 │\n",
      "│ episode_reward_max                                                                     200. │\n",
      "│ episode_reward_mean                                                                82.32984 │\n",
      "│ episode_reward_min                                                                      15. │\n",
      "│ episodes_this_iter                                                                      191 │\n",
      "│ hist_stats/episode_lengths                                             ...45, 43, 182, 172] │\n",
      "│ hist_stats/episode_reward                                              ....0, 182.0, 172.0] │\n",
      "│ info/learner/default_policy/diff_num_grad_updates_vs_sampler_policy                   967.0 │\n",
      "│ info/learner/default_policy/learner_stats/allreduce_latency                             0.0 │\n",
      "│ info/learner/default_policy/learner_stats/cur_lr                                     0.0001 │\n",
      "│ info/learner/default_policy/learner_stats/entropy_coeff                                0.01 │\n",
      "│ info/learner/default_policy/learner_stats/grad_gnorm                                   40.0 │\n",
      "│ info/learner/default_policy/learner_stats/policy_entropy                 19.880189895629883 │\n",
      "│ info/learner/default_policy/learner_stats/policy_loss                     46.44832229614258 │\n",
      "│ info/learner/default_policy/learner_stats/vf_loss                         878.4132690429688 │\n",
      "│ info/learner/default_policy/num_agent_steps_trained                                    32.0 │\n",
      "│ info/learner/default_policy/num_grad_updates_lifetime                                 968.0 │\n",
      "│ info/num_agent_steps_sampled                                                          30976 │\n",
      "│ info/num_agent_steps_trained                                                          30976 │\n",
      "│ info/num_env_steps_sampled                                                            30976 │\n",
      "│ info/num_env_steps_trained                                                            30976 │\n",
      "│ num_agent_steps_sampled                                                               30976 │\n",
      "│ num_agent_steps_trained                                                               30976 │\n",
      "│ num_env_steps_sampled                                                                 30976 │\n",
      "│ num_env_steps_sampled_this_iter                                                       15776 │\n",
      "│ num_env_steps_sampled_throughput_per_sec                                          1574.5699 │\n",
      "│ num_env_steps_trained                                                                 30976 │\n",
      "│ num_env_steps_trained_this_iter                                                       15776 │\n",
      "│ num_env_steps_trained_throughput_per_sec                                          1574.5699 │\n",
      "│ num_faulty_episodes                                                                       0 │\n",
      "│ num_healthy_workers                                                                       2 │\n",
      "│ num_in_flight_async_reqs                                                                  0 │\n",
      "│ num_remote_worker_restarts                                                                0 │\n",
      "│ num_steps_trained_this_iter                                                           15776 │\n",
      "│ perf/cpu_util_percent                                                     42.15714285714286 │\n",
      "│ perf/ram_util_percent                                                     96.31428571428572 │\n",
      "│ sampler_perf/mean_action_processing_ms                                  0.06812780182808327 │\n",
      "│ sampler_perf/mean_env_render_ms                                                         0.0 │\n",
      "│ sampler_perf/mean_env_wait_ms                                           0.03271397674357423 │\n",
      "│ sampler_perf/mean_inference_ms                                           0.4393308251247589 │\n",
      "│ sampler_perf/mean_raw_obs_processing_ms                                  0.2327684598188971 │\n",
      "│ sampler_results/connector_metrics/ObsPreprocessorConnector_ms         0.0030443930501089047 │\n",
      "│ sampler_results/connector_metrics/StateBufferConnector_ms              0.002707611203817797 │\n",
      "│ sampler_results/connector_metrics/ViewRequirementAgentConnector_ms      0.06659342980509653 │\n",
      "│ sampler_results/episode_len_mean                                          82.32984293193718 │\n",
      "│ sampler_results/episode_reward_max                                                    200.0 │\n",
      "│ sampler_results/episode_reward_mean                                       82.32984293193718 │\n",
      "│ sampler_results/episode_reward_min                                                     15.0 │\n",
      "│ sampler_results/episodes_this_iter                                                      191 │\n",
      "│ sampler_results/hist_stats/episode_lengths                             ...45, 43, 182, 172] │\n",
      "│ sampler_results/hist_stats/episode_reward                              ....0, 182.0, 172.0] │\n",
      "│ sampler_results/num_faulty_episodes                                                       0 │\n",
      "│ sampler_results/sampler_perf/mean_action_processing_ms                  0.06812780182808327 │\n",
      "│ sampler_results/sampler_perf/mean_env_render_ms                                         0.0 │\n",
      "│ sampler_results/sampler_perf/mean_env_wait_ms                           0.03271397674357423 │\n",
      "│ sampler_results/sampler_perf/mean_inference_ms                           0.4393308251247589 │\n",
      "│ sampler_results/sampler_perf/mean_raw_obs_processing_ms                  0.2327684598188971 │\n",
      "│ timers/learn_throughput                                                            9978.568 │\n",
      "│ timers/learn_time_ms                                                                  3.207 │\n",
      "│ timers/load_throughput                                                           209944.827 │\n",
      "│ timers/load_time_ms                                                                   0.152 │\n",
      "│ timers/sample_time_ms                                                                15.295 │\n",
      "│ timers/synch_weights_time_ms                                                          1.306 │\n",
      "│ timers/training_iteration_time_ms                                                    20.004 │\n",
      "╰─────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-25 09:37:32,014 E 46119 13267253] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-08-25_09-36-59_703066_46101 is over 95% full, available space: 11535552512; capacity: 245107195904. Object creation will fail if spilling is required.\n",
      "Trial status: 1 RUNNING\n",
      "Current time: 2023-08-25 09:37:35. Total running time: 30s\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Trial name                    status       iter     total time (s)      ts     reward     episode_reward_max     episode_reward_min     episode_len_mean     episodes_this_iter │\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ A2C_CartPole-v0_7aa12_00000   RUNNING         2            20.0336   30976    82.3298                    200                     15              82.3298                    191 │\n",
      "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n",
      "Training finished iteration 3 at 2023-08-25 09:37:41. Total running time: 36s\n",
      "╭─────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training result                                                                             │\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ episodes_total                                                                          697 │\n",
      "│ time_this_iter_s                                                                   10.01708 │\n",
      "│ time_total_s                                                                        30.0507 │\n",
      "│ timesteps_total                                                                       46624 │\n",
      "│ training_iteration                                                                        3 │\n",
      "│ agent_timesteps_total                                                                 46624 │\n",
      "│ connector_metrics/ObsPreprocessorConnector_ms                         0.0029718482887351907 │\n",
      "│ connector_metrics/StateBufferConnector_ms                             0.0026122554317935484 │\n",
      "│ connector_metrics/ViewRequirementAgentConnector_ms                      0.06475147310194078 │\n",
      "│ counters/num_agent_steps_sampled                                                      46624 │\n",
      "│ counters/num_agent_steps_trained                                                      46624 │\n",
      "│ counters/num_env_steps_sampled                                                        46624 │\n",
      "│ counters/num_env_steps_trained                                                        46624 │\n",
      "│ episode_len_mean                                                                   85.99451 │\n",
      "│ episode_reward_max                                                                     200. │\n",
      "│ episode_reward_mean                                                                85.99451 │\n",
      "│ episode_reward_min                                                                      11. │\n",
      "│ episodes_this_iter                                                                      182 │\n",
      "│ hist_stats/episode_lengths                                             ...15, 166, 63, 127] │\n",
      "│ hist_stats/episode_reward                                              ...6.0, 63.0, 127.0] │\n",
      "│ info/learner/default_policy/diff_num_grad_updates_vs_sampler_policy                  1456.0 │\n",
      "│ info/learner/default_policy/learner_stats/allreduce_latency                             0.0 │\n",
      "│ info/learner/default_policy/learner_stats/cur_lr                                     0.0001 │\n",
      "│ info/learner/default_policy/learner_stats/entropy_coeff                                0.01 │\n",
      "│ info/learner/default_policy/learner_stats/grad_gnorm                                   40.0 │\n",
      "│ info/learner/default_policy/learner_stats/policy_entropy                 17.838909149169922 │\n",
      "│ info/learner/default_policy/learner_stats/policy_loss                   -14.152074813842773 │\n",
      "│ info/learner/default_policy/learner_stats/vf_loss                            2222.400390625 │\n",
      "│ info/learner/default_policy/num_agent_steps_trained                                    32.0 │\n",
      "│ info/learner/default_policy/num_grad_updates_lifetime                                1457.0 │\n",
      "│ info/num_agent_steps_sampled                                                          46624 │\n",
      "│ info/num_agent_steps_trained                                                          46624 │\n",
      "│ info/num_env_steps_sampled                                                            46624 │\n",
      "│ info/num_env_steps_trained                                                            46624 │\n",
      "│ num_agent_steps_sampled                                                               46624 │\n",
      "│ num_agent_steps_trained                                                               46624 │\n",
      "│ num_env_steps_sampled                                                                 46624 │\n",
      "│ num_env_steps_sampled_this_iter                                                       15648 │\n",
      "│ num_env_steps_sampled_throughput_per_sec                                         1563.06015 │\n",
      "│ num_env_steps_trained                                                                 46624 │\n",
      "│ num_env_steps_trained_this_iter                                                       15648 │\n",
      "│ num_env_steps_trained_throughput_per_sec                                         1563.06015 │\n",
      "│ num_faulty_episodes                                                                       0 │\n",
      "│ num_healthy_workers                                                                       2 │\n",
      "│ num_in_flight_async_reqs                                                                  0 │\n",
      "│ num_remote_worker_restarts                                                                0 │\n",
      "│ num_steps_trained_this_iter                                                           15648 │\n",
      "│ perf/cpu_util_percent                                                     37.20714285714286 │\n",
      "│ perf/ram_util_percent                                                     96.07142857142857 │\n",
      "│ sampler_perf/mean_action_processing_ms                                  0.06745921922702618 │\n",
      "│ sampler_perf/mean_env_render_ms                                                         0.0 │\n",
      "│ sampler_perf/mean_env_wait_ms                                          0.032235806714315925 │\n",
      "│ sampler_perf/mean_inference_ms                                           0.4384410287741452 │\n",
      "│ sampler_perf/mean_raw_obs_processing_ms                                 0.22962114011510654 │\n",
      "│ sampler_results/connector_metrics/ObsPreprocessorConnector_ms         0.0029718482887351907 │\n",
      "│ sampler_results/connector_metrics/StateBufferConnector_ms             0.0026122554317935484 │\n",
      "│ sampler_results/connector_metrics/ViewRequirementAgentConnector_ms      0.06475147310194078 │\n",
      "│ sampler_results/episode_len_mean                                          85.99450549450549 │\n",
      "│ sampler_results/episode_reward_max                                                    200.0 │\n",
      "│ sampler_results/episode_reward_mean                                       85.99450549450549 │\n",
      "│ sampler_results/episode_reward_min                                                     11.0 │\n",
      "│ sampler_results/episodes_this_iter                                                      182 │\n",
      "│ sampler_results/hist_stats/episode_lengths                             ...15, 166, 63, 127] │\n",
      "│ sampler_results/hist_stats/episode_reward                              ...6.0, 63.0, 127.0] │\n",
      "│ sampler_results/num_faulty_episodes                                                       0 │\n",
      "│ sampler_results/sampler_perf/mean_action_processing_ms                  0.06745921922702618 │\n",
      "│ sampler_results/sampler_perf/mean_env_render_ms                                         0.0 │\n",
      "│ sampler_results/sampler_perf/mean_env_wait_ms                          0.032235806714315925 │\n",
      "│ sampler_results/sampler_perf/mean_inference_ms                           0.4384410287741452 │\n",
      "│ sampler_results/sampler_perf/mean_raw_obs_processing_ms                 0.22962114011510654 │\n",
      "│ timers/learn_throughput                                                            9620.791 │\n",
      "│ timers/learn_time_ms                                                                  3.326 │\n",
      "│ timers/load_throughput                                                           206615.961 │\n",
      "│ timers/load_time_ms                                                                   0.155 │\n",
      "│ timers/sample_time_ms                                                                16.633 │\n",
      "│ timers/synch_weights_time_ms                                                          1.388 │\n",
      "│ timers/training_iteration_time_ms                                                    21.549 │\n",
      "╰─────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-25 09:37:42,096 E 46119 13267253] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-08-25_09-36-59_703066_46101 is over 95% full, available space: 11515834368; capacity: 245107195904. Object creation will fail if spilling is required.\n",
      "Training finished iteration 4 at 2023-08-25 09:37:51. Total running time: 46s\n",
      "╭─────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training result                                                                             │\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ episodes_total                                                                          859 │\n",
      "│ time_this_iter_s                                                                   10.01016 │\n",
      "│ time_total_s                                                                       40.06086 │\n",
      "│ timesteps_total                                                                       61984 │\n",
      "│ training_iteration                                                                        4 │\n",
      "│ agent_timesteps_total                                                                 61984 │\n",
      "│ connector_metrics/ObsPreprocessorConnector_ms                         0.0034190990306712963 │\n",
      "│ connector_metrics/StateBufferConnector_ms                             0.0027471118503146702 │\n",
      "│ connector_metrics/ViewRequirementAgentConnector_ms                      0.06884586663893712 │\n",
      "│ counters/num_agent_steps_sampled                                                      61984 │\n",
      "│ counters/num_agent_steps_trained                                                      61984 │\n",
      "│ counters/num_env_steps_sampled                                                        61984 │\n",
      "│ counters/num_env_steps_trained                                                        61984 │\n",
      "│ episode_len_mean                                                                   94.59259 │\n",
      "│ episode_reward_max                                                                     200. │\n",
      "│ episode_reward_mean                                                                94.59259 │\n",
      "│ episode_reward_min                                                                      16. │\n",
      "│ episodes_this_iter                                                                      162 │\n",
      "│ hist_stats/episode_lengths                                             ...107, 16, 21, 107] │\n",
      "│ hist_stats/episode_reward                                              ...6.0, 21.0, 107.0] │\n",
      "│ info/learner/default_policy/diff_num_grad_updates_vs_sampler_policy                  1936.0 │\n",
      "│ info/learner/default_policy/learner_stats/allreduce_latency                             0.0 │\n",
      "│ info/learner/default_policy/learner_stats/cur_lr                                     0.0001 │\n",
      "│ info/learner/default_policy/learner_stats/entropy_coeff                                0.01 │\n",
      "│ info/learner/default_policy/learner_stats/grad_gnorm                                   40.0 │\n",
      "│ info/learner/default_policy/learner_stats/policy_entropy                 20.746679306030273 │\n",
      "│ info/learner/default_policy/learner_stats/policy_loss                       112.87744140625 │\n",
      "│ info/learner/default_policy/learner_stats/vf_loss                         635.1280517578125 │\n",
      "│ info/learner/default_policy/num_agent_steps_trained                                    32.0 │\n",
      "│ info/learner/default_policy/num_grad_updates_lifetime                                1937.0 │\n",
      "│ info/num_agent_steps_sampled                                                          61984 │\n",
      "│ info/num_agent_steps_trained                                                          61984 │\n",
      "│ info/num_env_steps_sampled                                                            61984 │\n",
      "│ info/num_env_steps_trained                                                            61984 │\n",
      "│ num_agent_steps_sampled                                                               61984 │\n",
      "│ num_agent_steps_trained                                                               61984 │\n",
      "│ num_env_steps_sampled                                                                 61984 │\n",
      "│ num_env_steps_sampled_this_iter                                                       15360 │\n",
      "│ num_env_steps_sampled_throughput_per_sec                                         1535.09153 │\n",
      "│ num_env_steps_trained                                                                 61984 │\n",
      "│ num_env_steps_trained_this_iter                                                       15360 │\n",
      "│ num_env_steps_trained_throughput_per_sec                                         1535.09153 │\n",
      "│ num_faulty_episodes                                                                       0 │\n",
      "│ num_healthy_workers                                                                       2 │\n",
      "│ num_in_flight_async_reqs                                                                  0 │\n",
      "│ num_remote_worker_restarts                                                                0 │\n",
      "│ num_steps_trained_this_iter                                                           15360 │\n",
      "│ perf/cpu_util_percent                                                     44.06666666666667 │\n",
      "│ perf/ram_util_percent                                                     96.39333333333335 │\n",
      "│ sampler_perf/mean_action_processing_ms                                   0.0678769079778855 │\n",
      "│ sampler_perf/mean_env_render_ms                                                         0.0 │\n",
      "│ sampler_perf/mean_env_wait_ms                                          0.032628224644895565 │\n",
      "│ sampler_perf/mean_inference_ms                                           0.4403214096053308 │\n",
      "│ sampler_perf/mean_raw_obs_processing_ms                                  0.2301623629755974 │\n",
      "│ sampler_results/connector_metrics/ObsPreprocessorConnector_ms         0.0034190990306712963 │\n",
      "│ sampler_results/connector_metrics/StateBufferConnector_ms             0.0027471118503146702 │\n",
      "│ sampler_results/connector_metrics/ViewRequirementAgentConnector_ms      0.06884586663893712 │\n",
      "│ sampler_results/episode_len_mean                                           94.5925925925926 │\n",
      "│ sampler_results/episode_reward_max                                                    200.0 │\n",
      "│ sampler_results/episode_reward_mean                                        94.5925925925926 │\n",
      "│ sampler_results/episode_reward_min                                                     16.0 │\n",
      "│ sampler_results/episodes_this_iter                                                      162 │\n",
      "│ sampler_results/hist_stats/episode_lengths                             ...107, 16, 21, 107] │\n",
      "│ sampler_results/hist_stats/episode_reward                              ...6.0, 21.0, 107.0] │\n",
      "│ sampler_results/num_faulty_episodes                                                       0 │\n",
      "│ sampler_results/sampler_perf/mean_action_processing_ms                   0.0678769079778855 │\n",
      "│ sampler_results/sampler_perf/mean_env_render_ms                                         0.0 │\n",
      "│ sampler_results/sampler_perf/mean_env_wait_ms                          0.032628224644895565 │\n",
      "│ sampler_results/sampler_perf/mean_inference_ms                           0.4403214096053308 │\n",
      "│ sampler_results/sampler_perf/mean_raw_obs_processing_ms                  0.2301623629755974 │\n",
      "│ timers/learn_throughput                                                           10147.638 │\n",
      "│ timers/learn_time_ms                                                                  3.153 │\n",
      "│ timers/load_throughput                                                           210339.646 │\n",
      "│ timers/load_time_ms                                                                   0.152 │\n",
      "│ timers/sample_time_ms                                                                 15.61 │\n",
      "│ timers/synch_weights_time_ms                                                          1.289 │\n",
      "│ timers/training_iteration_time_ms                                                    20.248 │\n",
      "╰─────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-25 09:37:52,193 E 46119 13267253] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-08-25_09-36-59_703066_46101 is over 95% full, available space: 11505827840; capacity: 245107195904. Object creation will fail if spilling is required.\n",
      "^C\n",
      "2023-08-25 09:37:52,398\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "Trial status: 1 RUNNING\n",
      "Current time: 2023-08-25 09:37:52. Total running time: 47s\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Trial name                    status       iter     total time (s)      ts     reward     episode_reward_max     episode_reward_min     episode_len_mean     episodes_this_iter │\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ A2C_CartPole-v0_7aa12_00000   RUNNING         4            40.0609   61984    94.5926                    200                     16              94.5926                    162 │\n",
      "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rllib train --run=A2C --env=CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 09:38:20,648\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "2023-08-25 09:38:20,687\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 09:38:24,392\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/tune/experiment/experiment.py:167: UserWarning: The `local_dir` argument of `Experiment is deprecated. Use `storage_path` or set the `TUNE_RESULT_DIR` environment variable instead.\n",
      "  warnings.warn(\n",
      "2023-08-25 09:38:25,818\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 09:38:25,823\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 09:38:25,878\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-08-25 09:38:25,985\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-25 09:38:26,009\tINFO tune.py:666 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2023-08-25 09:38:26,009\tWARNING syncer.py:260 -- You are using remote storage, but you don't have `fsspec` installed. This can lead to inefficient syncing behavior. To avoid this, install fsspec with `pip install fsspec`. Depending on your remote storage provider, consider installing the respective fsspec-package (see https://github.com/fsspec).\n",
      "2023-08-25 09:38:26,010\tINFO tensorboardx.py:178 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-08-25 09:38:26,010\tWARNING callback.py:144 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
      "╭────────────────────────────────────────────────────────╮\n",
      "│ Configuration for experiment     default               │\n",
      "├────────────────────────────────────────────────────────┤\n",
      "│ Search algorithm                 BasicVariantGenerator │\n",
      "│ Scheduler                        FIFOScheduler         │\n",
      "│ Number of trials                 1                     │\n",
      "╰────────────────────────────────────────────────────────╯\n",
      "\n",
      "View detailed results here: /Users/alishehper/ray_results/default\n",
      "\n",
      "Trial status: 1 PENDING\n",
      "Current time: 2023-08-25 09:38:26. Total running time: 0s\n",
      "Logical resource usage: 3.0/8 CPUs, 0/0 GPUs\n",
      "╭───────────────────────────────────────────────────╮\n",
      "│ Trial name                               status   │\n",
      "├───────────────────────────────────────────────────┤\n",
      "│ A2C_BreakoutNoFrameskip-v4_aaf93_00000   PENDING  │\n",
      "╰───────────────────────────────────────────────────╯\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=46312)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=46312)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m 2023-08-25 09:38:29,548\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m 2023-08-25 09:38:29,548\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m 2023-08-25 09:38:29,548\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py\", line 56, in step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py\", line 47, in step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     return env_step_passive_checker(self.env, action)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py\", line 237, in env_step_passive_checker\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     result = env.step(action)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/shimmy/atari_env.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     is_terminal = self.ale.game_over(with_truncation=False)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m TypeError: game_over(): incompatible function arguments. The following argument types are supported:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     1. (self: ale_py._ale_py.ALEInterface) -> bool\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m Invoked with: <ale_py._ale_py.ALEInterface object at 0x7f7c185b7bf0>; kwargs: with_truncation=False\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 235, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 404, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     check_env(self.env, self.config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 233, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     results = env.step(sampled_action)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py\", line 56, in step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     return self.env.step(action)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py\", line 47, in step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     return env_step_passive_checker(self.env, action)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py\", line 237, in env_step_passive_checker\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     result = env.step(action)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/shimmy/atari_env.py\", line 295, in step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     is_terminal = self.ale.game_over(with_truncation=False)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m TypeError: game_over(): incompatible function arguments. The following argument types are supported:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     1. (self: ale_py._ale_py.ALEInterface) -> bool\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m Invoked with: <ale_py._ale_py.ALEInterface object at 0x7f7c185b7bf0>; kwargs: with_truncation=False\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 235, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46318)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m 2023-08-25 09:38:32,602\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m 2023-08-25 09:38:32,603\tERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46320, ip=127.0.0.1, actor_id=ae5cca823e8c49aff2c3b79801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7a38c9af70>)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::A2C.__init__()\u001b[39m (pid=46312, ip=127.0.0.1, actor_id=5af3bcfa72bdef9f805a936901000000, repr=A2C)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 227, in _setup\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     self.add_workers(\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 593, in add_workers\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     raise result.get()\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 481, in __fetch_result\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     result = ray.get(r)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/algorithms/a2c/a2c.py\", line 165, in setup\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     super().setup(config)\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(A2C pid=46312)\u001b[0m     raise e.args[0].args[2]\n",
      "2023-08-25 09:38:32,671\tERROR tune_controller.py:911 -- Trial task failed for trial A2C_BreakoutNoFrameskip-v4_aaf93_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/_private/worker.py\", line 2522, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::A2C.__init__()\u001b[39m (pid=46312, ip=127.0.0.1, actor_id=5af3bcfa72bdef9f805a936901000000, repr=A2C)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 227, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 593, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 481, in __fetch_result\n",
      "    result = ray.get(r)\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py\", line 56, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py\", line 47, in step\n",
      "    return env_step_passive_checker(self.env, action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py\", line 237, in env_step_passive_checker\n",
      "    result = env.step(action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/shimmy/atari_env.py\", line 295, in step\n",
      "    is_terminal = self.ale.game_over(with_truncation=False)\n",
      "TypeError: game_over(): incompatible function arguments. The following argument types are supported:\n",
      "    1. (self: ale_py._ale_py.ALEInterface) -> bool\n",
      "\n",
      "Invoked with: <ale_py._ale_py.ALEInterface object at 0x7f7c185b7bf0>; kwargs: with_truncation=False\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 235, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 404, in __init__\n",
      "    check_env(self.env, self.config)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n",
      "    raise ValueError(\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 233, in check_gym_environments\n",
      "    results = env.step(sampled_action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py\", line 56, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py\", line 47, in step\n",
      "    return env_step_passive_checker(self.env, action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py\", line 237, in env_step_passive_checker\n",
      "    result = env.step(action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/shimmy/atari_env.py\", line 295, in step\n",
      "    is_terminal = self.ale.game_over(with_truncation=False)\n",
      "TypeError: game_over(): incompatible function arguments. The following argument types are supported:\n",
      "    1. (self: ale_py._ale_py.ALEInterface) -> bool\n",
      "\n",
      "Invoked with: <ale_py._ale_py.ALEInterface object at 0x7f7c185b7bf0>; kwargs: with_truncation=False\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46318, ip=127.0.0.1, actor_id=cf34cae5607eb1e1f1c7eb7301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7c2b523ca0>)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 235, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::A2C.__init__()\u001b[39m (pid=46312, ip=127.0.0.1, actor_id=5af3bcfa72bdef9f805a936901000000, repr=A2C)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/algorithms/a2c/a2c.py\", line 165, in setup\n",
      "    super().setup(config)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 179, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 233, in check_gym_environments\n",
      "    results = env.step(sampled_action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py\", line 56, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py\", line 47, in step\n",
      "    return env_step_passive_checker(self.env, action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py\", line 237, in env_step_passive_checker\n",
      "    result = env.step(action)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/shimmy/atari_env.py\", line 295, in step\n",
      "    is_terminal = self.ale.game_over(with_truncation=False)\n",
      "TypeError: game_over(): incompatible function arguments. The following argument types are supported:\n",
      "    1. (self: ale_py._ale_py.ALEInterface) -> bool\n",
      "\n",
      "Invoked with: <ale_py._ale_py.ALEInterface object at 0x7f7c185b7bf0>; kwargs: with_truncation=False\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::A2C.__init__()\u001b[39m (pid=46312, ip=127.0.0.1, actor_id=5af3bcfa72bdef9f805a936901000000, repr=A2C)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "  File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 235, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "Trial status: 1 ERROR\n",
      "Current time: 2023-08-25 09:38:32. Total running time: 6s\n",
      "Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
      "╭───────────────────────────────────────────────────╮\n",
      "│ Trial name                               status   │\n",
      "├───────────────────────────────────────────────────┤\n",
      "│ A2C_BreakoutNoFrameskip-v4_aaf93_00000   ERROR    │\n",
      "╰───────────────────────────────────────────────────╯\n",
      "\n",
      "Number of errored trials: 1\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Trial name                                 # failures   error file                                                                                                   │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ A2C_BreakoutNoFrameskip-v4_aaf93_00000              1   /Users/alishehper/ray_results/default/A2C_BreakoutNoFrameskip-v4_aaf93_00000_0_2023-08-25_09-38-26/error.txt │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rll\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33mib/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m293\u001b[0m in \u001b[92mrun\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   │   \u001b[0m}                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m293 \u001b[2m│   │   \u001b[0mrun_rllib_experiments(                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │   │   \u001b[0mexperiments=experiments,                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   │   │   \u001b[0mv=v,                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m296 \u001b[0m\u001b[2m│   │   │   \u001b[0mvv=vv,                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────── locals ────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    algo = \u001b[33m'A2C'\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       checkpoint_at_end = \u001b[94mTrue\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         checkpoint_freq = \u001b[94m0\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_score_attr = \u001b[33m'training_iteration'\u001b[0m                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     ctx = \u001b[1m<\u001b[0m\u001b[1;95mclick.core.Context\u001b[0m\u001b[39m object at \u001b[0m\u001b[94m0x7fe0e3f50a30\u001b[0m\u001b[1m>\u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     env = \u001b[33m'BreakoutNoFrameskip-v4'\u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         experiment_name = \u001b[33m'default'\u001b[0m                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             experiments = \u001b[1m{\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'default'\u001b[0m: \u001b[1m{\u001b[0m                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'run'\u001b[0m: \u001b[33m'A2C'\u001b[0m,                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'checkpoint_config'\u001b[0m:                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcheckpoint_score_attribute\u001b[0m=\u001b[33m'…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33mcheckpoint_at_end\u001b[0m=\u001b[94mTrue\u001b[0m\u001b[1m)\u001b[0m,                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'local_dir'\u001b[0m:                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33m'/Users/alishehper/ray_results'\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'resources_per_trial'\u001b[0m: \u001b[94mNone\u001b[0m,           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'stop'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'config'\u001b[0m: \u001b[1m{\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'env'\u001b[0m: \u001b[33m'BreakoutNoFrameskip-v4'\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[1m}\u001b[0m,                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'restore'\u001b[0m: \u001b[94mNone\u001b[0m,                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'num_samples'\u001b[0m: \u001b[94m1\u001b[0m,                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'sync_config'\u001b[0m: \u001b[1;35mSyncConfig\u001b[0m\u001b[1m(\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33mupload_dir\u001b[0m=\u001b[33m''\u001b[0m,                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msyncer\u001b[0m=\u001b[33m'auto'\u001b[0m,                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_period\u001b[0m=\u001b[94m300\u001b[0m,                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_timeout\u001b[0m=\u001b[94m1800\u001b[0m,                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_artifacts\u001b[0m=\u001b[94mTrue\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_on_checkpoint\u001b[0m=\u001b[94mTrue\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[1m)\u001b[0m                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1m}\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               framework = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    keep_checkpoints_num = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               local_dir = \u001b[33m'/Users/alishehper/ray_results'\u001b[0m                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_mode = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             num_samples = \u001b[94m1\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             ray_address = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_cpus = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_gpus = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           ray_num_nodes = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m ray_object_store_memory = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  ray_ui = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     resources_per_trial = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 restore = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  resume = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               scheduler = \u001b[33m'FIFO'\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        scheduler_config = \u001b[33m'\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m'\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    stop = \u001b[33m'\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[33m'\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   trace = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              upload_dir = \u001b[33m''\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       v = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      vv = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rll\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33mib/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m389\u001b[0m in \u001b[92mrun_rllib_experiments\u001b[0m                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m386 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m387 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Run the Tune experiment and return the trials.\u001b[0m                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m388 \u001b[0m\u001b[2m│   \u001b[0mscheduler_config = json.loads(scheduler_config)                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m389 \u001b[2m│   \u001b[0mtrials = run_experiments(                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m390 \u001b[0m\u001b[2m│   │   \u001b[0mexperiments,                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m391 \u001b[0m\u001b[2m│   │   \u001b[0mscheduler=create_scheduler(scheduler, **scheduler_config),     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m392 \u001b[0m\u001b[2m│   │   \u001b[0mresume=resume,                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────── locals ────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    algo = \u001b[33m'A2C'\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     exp = \u001b[1m{\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'run'\u001b[0m: \u001b[33m'A2C'\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'checkpoint_config'\u001b[0m:                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcheckpoint_score_attribute\u001b[0m=\u001b[33m'…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33mcheckpoint_at_end\u001b[0m=\u001b[94mTrue\u001b[0m\u001b[1m)\u001b[0m,                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'local_dir'\u001b[0m:                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33m'/Users/alishehper/ray_results'\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'resources_per_trial'\u001b[0m: \u001b[94mNone\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'stop'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'config'\u001b[0m: \u001b[1m{\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'env'\u001b[0m: \u001b[33m'BreakoutNoFrameskip-v4'\u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m,                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'restore'\u001b[0m: \u001b[94mNone\u001b[0m,                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'num_samples'\u001b[0m: \u001b[94m1\u001b[0m,                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'sync_config'\u001b[0m: \u001b[1;35mSyncConfig\u001b[0m\u001b[1m(\u001b[0m                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33mupload_dir\u001b[0m=\u001b[33m''\u001b[0m,                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33msyncer\u001b[0m=\u001b[33m'auto'\u001b[0m,                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33msync_period\u001b[0m=\u001b[94m300\u001b[0m,                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33msync_timeout\u001b[0m=\u001b[94m1800\u001b[0m,                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33msync_artifacts\u001b[0m=\u001b[94mTrue\u001b[0m,                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33msync_on_checkpoint\u001b[0m=\u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[1m)\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1m}\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             experiments = \u001b[1m{\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[33m'default'\u001b[0m: \u001b[1m{\u001b[0m                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'run'\u001b[0m: \u001b[33m'A2C'\u001b[0m,                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'checkpoint_config'\u001b[0m:                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcheckpoint_score_attribute\u001b[0m=\u001b[33m'…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33mcheckpoint_at_end\u001b[0m=\u001b[94mTrue\u001b[0m\u001b[1m)\u001b[0m,                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'local_dir'\u001b[0m:                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[33m'/Users/alishehper/ray_results'\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'resources_per_trial'\u001b[0m: \u001b[94mNone\u001b[0m,           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'stop'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'config'\u001b[0m: \u001b[1m{\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33m'env'\u001b[0m: \u001b[33m'BreakoutNoFrameskip-v4'\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[1m}\u001b[0m,                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'restore'\u001b[0m: \u001b[94mNone\u001b[0m,                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'num_samples'\u001b[0m: \u001b[94m1\u001b[0m,                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[33m'sync_config'\u001b[0m: \u001b[1;35mSyncConfig\u001b[0m\u001b[1m(\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33mupload_dir\u001b[0m=\u001b[33m''\u001b[0m,                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msyncer\u001b[0m=\u001b[33m'auto'\u001b[0m,                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_period\u001b[0m=\u001b[94m300\u001b[0m,                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_timeout\u001b[0m=\u001b[94m1800\u001b[0m,                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_artifacts\u001b[0m=\u001b[94mTrue\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   │   \u001b[0m\u001b[33msync_on_checkpoint\u001b[0m=\u001b[94mTrue\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   │   \u001b[0m\u001b[1m)\u001b[0m                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           \u001b[1m}\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               framework = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  input_ = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_mode = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             ray_address = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_cpus = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            ray_num_gpus = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           ray_num_nodes = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m ray_object_store_memory = \u001b[94mNone\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  ray_ui = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  resume = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               scheduler = \u001b[33m'FIFO'\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        scheduler_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   trace = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       v = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 verbose = \u001b[94m1\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      vv = \u001b[94mFalse\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/tun\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33me/\u001b[0m\u001b[1;33mtune.py\u001b[0m:\u001b[94m1258\u001b[0m in \u001b[92mrun_experiments\u001b[0m                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1255 \u001b[0m\u001b[2m│   \u001b[0mexperiments = _convert_to_experiment_list(experiments)            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1256 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1257 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m concurrent:                                                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1258 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m run(                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1259 \u001b[0m\u001b[2m│   │   │   \u001b[0mexperiments,                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1260 \u001b[0m\u001b[2m│   │   │   \u001b[0mserver_port=server_port,                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1261 \u001b[0m\u001b[2m│   │   │   \u001b[0mverbose=verbose,                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────── locals ────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               _remote = \u001b[94mFalse\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             callbacks = \u001b[94mNone\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            concurrent = \u001b[94mTrue\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           experiments = \u001b[1m[\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experiment\u001b[0m\u001b[39m \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5ca60\u001b[0m\u001b[1m>\u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[1m]\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     progress_reporter = \u001b[94mNone\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m raise_on_failed_trial = \u001b[94mTrue\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                resume = \u001b[94mFalse\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          reuse_actors = \u001b[94mNone\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             scheduler = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.schedulers.trial_scheduler.FIFOSchedu…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5cac0\u001b[0m\u001b[1m>\u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           server_port = \u001b[94mNone\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        trial_executor = \u001b[94mNone\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               verbose = \u001b[94m1\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/tun\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33me/\u001b[0m\u001b[1;33mtune.py\u001b[0m:\u001b[94m1142\u001b[0m in \u001b[92mrun\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1139 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m incomplete_trials:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m raise_on_failed_trial \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m experiment_interrupted_event \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1142 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m TuneError(\u001b[33m\"\u001b[0m\u001b[33mTrials did not complete\u001b[0m\u001b[33m\"\u001b[0m, incomplete_tri \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1143 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   │   \u001b[0mlogger.error(\u001b[33m\"\u001b[0m\u001b[33mTrials did not complete: \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m, incomplete_tr \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1145 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────── locals ────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    _entrypoint = \u001b[1m<\u001b[0m\u001b[1;95mAirEntrypoint.TUNE_RUN_EXPERIMENTS:\u001b[0m\u001b[39m \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33m'tune.run_experiments'\u001b[0m\u001b[1m>\u001b[0m                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     _experiment_checkpoint_dir = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        _remote = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           _remote_string_queue = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          air_progress_reporter = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.experimental.output.TuneTerm…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c016a070\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  air_verbosity = \u001b[94m1\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      all_start = \u001b[94m1692970706.008967\u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     all_trials = \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  A2C_BreakoutNoFrameskip-v4_aaf93_00000  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m]\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      callbacks = \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.experimental.output.AirResul…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014f910\u001b[0m\u001b[1m>\u001b[0m,              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.logger.csv.CSVLoggerCallback\u001b[0m\u001b[39m \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014f970\u001b[0m\u001b[1m>\u001b[0m,              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.logger.json.JsonLoggerCallba…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014f9d0\u001b[0m\u001b[1m>\u001b[0m,              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mray.tune.syncer.SyncerCallback\u001b[0m\u001b[39m \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014fa30\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m]\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             chdir_to_trial_dir = \u001b[94mTrue\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              checkpoint_at_end = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              checkpoint_config = \u001b[1;35mCheckpointConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                checkpoint_freq = \u001b[94m0\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      checkpoint_keep_all_ranks = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          checkpoint_score_attr = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m checkpoint_upload_from_workers = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              error_message_map = \u001b[1m{\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'entrypoint'\u001b[0m:                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33m'tune.run_experiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m'\u001b[0m,            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'search_space_arg'\u001b[0m:                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33m'\u001b[0m\u001b[33mexperiment\u001b[0m\u001b[33m=\u001b[0m\u001b[33mExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m'\u001b[0m,        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'restore_entrypoint'\u001b[0m:               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33m'tune.run_experiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33m..., \u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33mresume\u001b[0m\u001b[33m=\u001b[0m\u001b[33mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m'\u001b[0m                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                            exp = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experi…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5ca60\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          experiment_checkpoint = \u001b[33m'/Users/alishehper/ray_results/default…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   experiment_interrupted_event = \u001b[1m<\u001b[0m\u001b[1;95mthreading.Event\u001b[0m\u001b[39m object at \u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[94m0x7fe0c014fa90\u001b[0m\u001b[1m>\u001b[0m                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    experiments = \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experi…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5ca60\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m]\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 export_formats = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      fail_fast = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                              i = \u001b[94m0\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              incomplete_trials = \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  A2C_BreakoutNoFrameskip-v4_aaf93_00000  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m]\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  is_local_mode = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           keep_checkpoints_num = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      local_dir = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     local_path = \u001b[33m'/Users/alishehper/ray_results'\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    log_to_file = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          max_concurrent_trials = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   max_failures = \u001b[94m0\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         metric = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           mode = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           name = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    num_samples = \u001b[94m1\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          placeholder_resolvers = \u001b[1;35mdefaultdict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[33m'list'\u001b[0m\u001b[1m>\u001b[0m, \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               progress_metrics = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              progress_reporter = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          raise_on_failed_trial = \u001b[94mTrue\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    remote_path = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            resources_per_trial = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        restore = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           result_buffer_length = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         resume = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   reuse_actors = \u001b[94mFalse\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              run_or_experiment = \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experi…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5ca60\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m]\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         runner = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.execution.tune_controller.Tu…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014fb80\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  runner_kwargs = \u001b[1m{\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'search_alg'\u001b[0m:                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.search.basic_variant.BasicVa…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014f820\u001b[0m\u001b[1m>\u001b[0m,              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'placeholder_resolvers'\u001b[0m:            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1;35mdefaultdict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[33m'list'\u001b[0m\u001b[1m>\u001b[0m, \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'scheduler'\u001b[0m:                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.schedulers.trial_scheduler.F…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5cac0\u001b[0m\u001b[1m>\u001b[0m,              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'experiment_path'\u001b[0m:                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33m'/Users/alishehper/ray_results/default…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'experiment_dir_name'\u001b[0m: \u001b[33m'default'\u001b[0m,   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'sync_config'\u001b[0m: \u001b[1;35mSyncConfig\u001b[0m\u001b[1m(\u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   │   \u001b[0m\u001b[33mupload_dir\u001b[0m=\u001b[94mNone\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   │   \u001b[0m\u001b[33msyncer\u001b[0m=\u001b[33m'auto'\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   │   \u001b[0m\u001b[33msync_period\u001b[0m=\u001b[94m300\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   │   \u001b[0m\u001b[33msync_timeout\u001b[0m=\u001b[94m1800\u001b[0m,              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   │   \u001b[0m\u001b[33msync_artifacts\u001b[0m=\u001b[94mTrue\u001b[0m,            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   │   \u001b[0m\u001b[33msync_on_checkpoint\u001b[0m=\u001b[94mTrue\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[1m)\u001b[0m,                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'stopper'\u001b[0m: \u001b[94mNone\u001b[0m,                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'resume'\u001b[0m: \u001b[94mFalse\u001b[0m,                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'server_port'\u001b[0m: \u001b[94mNone\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m'fail_fast'\u001b[0m: \u001b[94mFalse\u001b[0m,                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33m...\u001b[0m +\u001b[94m6\u001b[0m                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      scheduler = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.schedulers.trial_scheduler.F…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5cac0\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     search_alg = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.search.basic_variant.BasicVa…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014f820\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    server_port = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          stack = \u001b[1m<\u001b[0m\u001b[1;95mcontextlib.ExitStack\u001b[0m\u001b[39m object at \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[94m0x7fe0c016a040\u001b[0m\u001b[1m>\u001b[0m                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           stop = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   storage_path = \u001b[33m'/Users/alishehper/ray_results'\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    sync_config = \u001b[1;35mSyncConfig\u001b[0m\u001b[1m(\u001b[0m                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33mupload_dir\u001b[0m=\u001b[94mNone\u001b[0m,                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33msyncer\u001b[0m=\u001b[33m'auto'\u001b[0m,                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33msync_period\u001b[0m=\u001b[94m300\u001b[0m,                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33msync_timeout\u001b[0m=\u001b[94m1800\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33msync_artifacts\u001b[0m=\u001b[94mTrue\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m\u001b[33msync_on_checkpoint\u001b[0m=\u001b[94mTrue\u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m)\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               tensorboard_path = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  time_budget_s = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      trainable = \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[2m│   \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m<\u001b[0m\u001b[1;95mray.tune.experiment.experiment.Experi…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0e3f5ca60\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[1m]\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          trial = A2C_BreakoutNoFrameskip-v4_aaf93_00000  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          trial_dirname_creator = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 trial_executor = \u001b[1m<\u001b[0m\u001b[1;95mray.tune.execution.ray_trial_executor…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[39mobject at \u001b[0m\u001b[94m0x7fe0c014fb50\u001b[0m\u001b[1m>\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trial_name_creator = \u001b[94mNone\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               trial_runner_cls = \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33m'ray.tune.execution.tune_controller.Tu…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     tune_start = \u001b[94m1692970706.014912\u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     tune_taken = \u001b[94m6.66113805770874\u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               TuneRichReporter = \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[33m'ray.tune.experimental.output.TuneRich…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        verbose = \u001b[94m1\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mTuneError: \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'Trials did not complete'\u001b[0m, \u001b[1m[\u001b[0mA2C_BreakoutNoFrameskip-v4_aaf93_00000\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "\u001b[0m\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-25 09:38:33,347 E 46298 13269286] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-08-25_09-38-21_330942_46288 is over 95% full, available space: 11534721024; capacity: 245107195904. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[36m(pid=46320)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=46320)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m [Powered by Stella]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46320, ip=127.0.0.1, actor_id=ae5cca823e8c49aff2c3b79801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7a38c9af70>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/shimmy/atari_env.py\", line 295, in step\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     return self.env.step(action)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     return env_step_passive_checker(self.env, action)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py\", line 237, in env_step_passive_checker\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     result = env.step(action)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     is_terminal = self.ale.game_over(with_truncation=False)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m TypeError: game_over(): incompatible function arguments. The following argument types are supported:\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     1. (self: ale_py._ale_py.ALEInterface) -> bool\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m \u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m Invoked with: <ale_py._ale_py.ALEInterface object at 0x7f7a5b2b0cb0>; kwargs: with_truncation=False\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m The above exception was the direct cause of the following exception:\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=46320, ip=127.0.0.1, actor_id=ae5cca823e8c49aff2c3b79801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7a38c9af70>)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     check_gym_environments(env, AlgorithmConfig() if config is None else config)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py\", line 235, in check_gym_environments\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     raise ValueError(\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m ValueError: Your gymnasium.Env's `step()` method raised an Exception!\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m   File \"/Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 404, in __init__\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     check_env(self.env, self.config)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m ValueError: Traceback (most recent call last):\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m     results = env.step(sampled_action)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=46320)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!rllib train --run=A2C --env=BreakoutNoFrameskip-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.rllib.agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mppo\u001b[39;00m \u001b[39mimport\u001b[39;00m PPOTrainer\n\u001b[1;32m      3\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mCartPole-v0\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Change the following line to `“framework”: “tf”` to use tensorflow\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     },\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m stop \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mepisode_reward_mean\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m195\u001b[39m}\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.rllib.agents'"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "config = {\n",
    "    \"env\": \"CartPole-v0\",\n",
    "    # Change the following line to `“framework”: “tf”` to use tensorflow\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "      \"fcnet_hiddens\": [32],\n",
    "      \"fcnet_activation\": \"linear\",\n",
    "    },\n",
    "}\n",
    "stop = {\"episode_reward_mean\": 195}\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "  num_cpus=3,\n",
    "  include_dashboard=False,\n",
    "  ignore_reinit_error=True,\n",
    "  log_to_driver=False,\n",
    ")\n",
    "# execute training \n",
    "analysis = ray.tune.run(\n",
    "  \"PPO\",\n",
    "  config=config,\n",
    "  stop=stop,\n",
    "  checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typer\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages (from typer) (8.1.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alishehper/work/RL-algorithms/env/lib/python3.9/site-packages (from typer) (4.5.0)\n",
      "Installing collected packages: typer\n",
      "Successfully installed typer-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
